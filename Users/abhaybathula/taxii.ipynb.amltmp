{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only libraries allowed are:\n",
        "# Scikit-learn, XGBoost, Imblearn, NumPy\n",
        "# Pandas, SciPy, Pickle, regex\n",
        "# Seaborn, Matplotlib, Lightgbm\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
        "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "        \n",
        "XYTrain = pd.read_csv('train.csv')\n",
        "XTest = pd.read_csv('test.csv') # naturally, there is no Y in the testing set\n",
        "\n",
        "def preprocess(df):\n",
        "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
        "    df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).abs().dt.total_seconds()\n",
        "    \n",
        "    df['passenger_count'].fillna(df['passenger_count'].mode()[0], inplace=True)\n",
        "    df['payment_type'].replace('unknown', 'Credit Card', inplace = True)\n",
        "    df['RatecodeID'].fillna(df['RatecodeID'].mode()[0], inplace=True)\n",
        "    df['congestion_surcharge'].fillna(0, inplace=True)\n",
        "    df['store_and_fwd_flag'].fillna('N',inplace=True)\n",
        "    df['Airport_fee'].fillna(0, inplace=True)\n",
        "    \n",
        "    df = pd.get_dummies(df, columns=['store_and_fwd_flag'], prefix=['store_and_fwd_flag'])\n",
        "    df = pd.get_dummies(df, columns=['payment_type'], prefix=['payment_type'])\n",
        "    \n",
        "    df['improvement_surcharge'] = df['improvement_surcharge'].abs()\n",
        "    df['congestion_surcharge'] = df['congestion_surcharge'].abs()\n",
        "    df['tolls_amount'] = df['tolls_amount'].abs()\n",
        "    df['Airport_fee'] = df['Airport_fee'].abs()\n",
        "    df['extra'] = df['extra'].abs()\n",
        "    \n",
        "    #df = df.drop(columns=['VendorID', 'RatecodeID', 'PULocationID', 'DOLocationID'])\n",
        "    \n",
        "    try:\n",
        "        df['total_amount'] = df['total_amount'].abs()\n",
        "        Y = df['total_amount']\n",
        "        df = df.drop(columns=['total_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime'])\n",
        "        \n",
        "        numericalCols = [x for x in df.select_dtypes(include = 'number').columns.to_list()] #if 'ID' not in x]\n",
        "        df[numericalCols] = MinMaxScaler().fit_transform(df[numericalCols])\n",
        "        \n",
        "        return train_test_split(df, Y, test_size=0.2, random_state=42)\n",
        "    except:\n",
        "        df = df.drop(columns=['tpep_dropoff_datetime', 'tpep_pickup_datetime'])\n",
        "        \n",
        "        numericalCols = [x for x in df.select_dtypes(include = 'number').columns.to_list()] #if 'ID' not in x]\n",
        "        df[numericalCols] = MinMaxScaler().fit_transform(df[numericalCols])\n",
        "        \n",
        "        return df\n",
        "\n",
        "XTest = preprocess(XTest)\n",
        "XTrain, XVal, YTrain, YVal = preprocess(XYTrain)\n",
        "\n",
        "best_models_raw = {}\n",
        "best_models_tuned = {}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1700988570294
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline LR Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    ('poly', PolynomialFeatures()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'poly__degree': [2, 3],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "grid_search.fit(XTrain, YTrain)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "YPred = best_model.predict(XVal)\n",
        "print(f'R2 score: {r2_score(YPred, YVal)}')\n",
        "print(f'Best params: {best_params}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "R2 score: 0.8772601173373252\nBest params: {'poly__degree': 2}\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700560207708
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Untuned Run"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressors = {\n",
        "    'Linear Regression': (LinearRegression(), {}),\n",
        "    'K-Nearest Neighbors Regressor': (KNeighborsRegressor(), {}),\n",
        "    'Support Vector Regressor': (SVR(), {}),\n",
        "    'Decision Tree Regressor': (DecisionTreeRegressor(), {}),\n",
        "    'Bagging Regressor': (BaggingRegressor(estimator=DecisionTreeRegressor()), {}),\n",
        "    'AdaBoost Regressor': (AdaBoostRegressor(estimator=DecisionTreeRegressor()), {}),\n",
        "    'Multi-Layer Perceptron Regressor': (MLPRegressor(), {})\n",
        "}\n",
        "\n",
        "for regressor_name, (regressor, param_grid) in regressors.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly', PolynomialFeatures(degree=2)),\n",
        "        ('regressor', regressor)\n",
        "    ])\n",
        "    \n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "    grid_search.fit(XTrain, YTrain)\n",
        "    \n",
        "    best_models_raw[regressor_name] = {\n",
        "        'model': grid_search.best_estimator_,\n",
        "        'params': grid_search.best_params_\n",
        "    }\n",
        "    \n",
        "    YPred = grid_search.best_estimator_.predict(XVal)\n",
        "\n",
        "r2_table_raw, i = pd.DataFrame(columns=['Model Name', 'R2 Score on Training','R2 Score on Validation', '% Difference in R2']), 0\n",
        "for regressor_name, results in best_models_raw.items():\n",
        "    YPredVal = results[\"model\"].predict(XVal)\n",
        "    YPredTrn = results[\"model\"].predict(XTrain)\n",
        "    r2_val = r2_score(YPredVal, YVal)\n",
        "    r2_trn = r2_score(YPredTrn, YTrain)\n",
        "    r2_table_raw.loc[i] = [regressor_name, r2_trn, r2_val, abs(r2_trn-r2_val)*100/abs(r2_trn)]\n",
        "    i=i+1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "LR\tmodel trained with an R-squared scores of 0.8774 on the validation set and 0.8736 on the training set.\nKNN\tmodel trained with an R-squared scores of 0.6824 on the validation set and 0.7921 on the training set.\nCART\tmodel trained with an R-squared scores of 0.9111 on the validation set and 1.0000 on the training set.\nMLP\tmodel trained with an R-squared scores of 0.8109 on the validation set and 0.4096 on the training set.\nBagging\tmodel trained with an R-squared scores of 0.9466 on the validation set and 0.9877 on the training set.\nBoosting\tmodel trained with an R-squared scores of 0.9490 on the validation set and 1.0000 on the training set.\nSVM\tmodel trained with an R-squared scores of -0.1158 on the validation set and -0.1123 on the training set.\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699766518250
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuned Run"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressors = {\n",
        "    #'Decision Tree Regressor (Tuned)': (DecisionTreeRegressor(), {'regressor__max_depth': [None, 5, 10, 20], 'regressor__ccp_alpha': [0.01, 0.1, 1]}),\n",
        "    #'Bagging Regressor (Tuned)': (BaggingRegressor(estimator=DecisionTreeRegressor()), {'regressor__n_estimators': [10, 50, 100]}),\n",
        "    'AdaBoost Regressor (Tuned)': (AdaBoostRegressor(estimator=DecisionTreeRegressor()), {'regressor__n_estimators': [10, 50, 100], 'regressor__learning_rate': [0.1, 1.0, 2.0], 'regressor__loss': ['linear', 'square', 'exponential']}),\n",
        "}\n",
        "\n",
        "for regressor_name, (regressor, param_grid) in regressors.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly', PolynomialFeatures(degree=2)),\n",
        "        ('regressor', regressor)\n",
        "    ])\n",
        "    \n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "    grid_search.fit(XTrain, YTrain)\n",
        "    \n",
        "    best_models_tuned[regressor_name] = {\n",
        "        'model': grid_search.best_estimator_,\n",
        "        'params': grid_search.best_params_\n",
        "    }\n",
        "    \n",
        "\n",
        "r2_table_tuned, i = pd.DataFrame(columns=['Model Name', 'R2 Score on Training','R2 Score on Validation', '% Difference in R2']), 0\n",
        "for regressor_name, results in best_models_tuned.items():\n",
        "    YPredVal = results[\"model\"].predict(XVal)\n",
        "    YPredTrn = results[\"model\"].predict(XTrain)\n",
        "    r2_val = r2_score(YPredVal, YVal)\n",
        "    r2_trn = r2_score(YPredTrn, YTrain)\n",
        "    r2_table_tuned.loc[i] = [regressor_name, r2_trn, r2_val, abs(r2_trn-r2_val)*100/abs(r2_trn)]\n",
        "    i=i+1\n",
        "    \n",
        "r2_table_tuned"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "                        Model Name  R2 Score on Training  \\\n2  Decision Tree Regressor (Tuned)              0.952982   \n3        Bagging Regressor (Tuned)              0.990570   \n4       AdaBoost Regressor (Tuned)              0.999911   \n\n   R2 Score on Validation  % Difference in R2  \n2                0.926811            2.746227  \n3                0.951213            3.973134  \n4                0.951514            4.840120  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Name</th>\n      <th>R2 Score on Training</th>\n      <th>R2 Score on Validation</th>\n      <th>% Difference in R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>Decision Tree Regressor (Tuned)</td>\n      <td>0.952982</td>\n      <td>0.926811</td>\n      <td>2.746227</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bagging Regressor (Tuned)</td>\n      <td>0.990570</td>\n      <td>0.951213</td>\n      <td>3.973134</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AdaBoost Regressor (Tuned)</td>\n      <td>0.999911</td>\n      <td>0.951514</td>\n      <td>4.840120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700581057561
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_models_tuned['AdaBoost Regressor (Tuned)']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "{'model': Pipeline(steps=[('scaler', StandardScaler()), ('poly', PolynomialFeatures()),\n                 ('regressor',\n                  AdaBoostRegressor(estimator=DecisionTreeRegressor(),\n                                    learning_rate=2.0, loss='exponential',\n                                    n_estimators=100))]),\n 'params': {'regressor__learning_rate': 2.0,\n  'regressor__loss': 'exponential',\n  'regressor__n_estimators': 100}}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700582572111
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XTrain = PolynomialFeatures(degree=2).fit_transform(XTrain)\n",
        "\n",
        "final_model = AdaBoostRegressor(estimator=DecisionTreeRegressor(),\n",
        "                          learning_rate=2.0, \n",
        "                          loss='exponential',\n",
        "                          n_estimators=100)\n",
        "\n",
        "final_model.fit(XTrain, YTrain)\n",
        "\n",
        "YPred = final_model.predict(PolynomialFeatures(degree=2).fit_transform(XTest))\n",
        "OUTdf = pd.DataFrame()\n",
        "OUTdf['ID'], OUTdf['total_amount'] = [x for x in range(1, XTest.shape[0]+1)], YPred\n",
        "\n",
        "OUTdf.to_csv('submission.csv', index = False)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m XTrain \u001b[38;5;241m=\u001b[39m \u001b[43mPolynomialFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXTrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m final_model \u001b[38;5;241m=\u001b[39m AdaBoostRegressor(estimator\u001b[38;5;241m=\u001b[39mDecisionTreeRegressor(),\n\u001b[1;32m      4\u001b[0m                           learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, \n\u001b[1;32m      5\u001b[0m                           loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexponential\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                           n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m final_model\u001b[38;5;241m.\u001b[39mfit(XTrain, YTrain)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/preprocessing/_polynomial.py:440\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# XP[:, start:end] are terms of degree d - 1\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# that exclude feature #feature_idx.\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXP\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXP\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_col\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnext_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     current_col \u001b[38;5;241m=\u001b[39m next_col\n\u001b[1;32m    448\u001b[0m new_index\u001b[38;5;241m.\u001b[39mappend(current_col)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700652812577
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressors = {\n",
        "    'Decision Tree Regressor (Tuned)': (DecisionTreeRegressor(), {'regressor__max_depth': [10], 'regressor__ccp_alpha': [0.01]}),\n",
        "    'Bagging Regressor (Tuned)': (BaggingRegressor(estimator=DecisionTreeRegressor()), {'regressor__n_estimators': [100]}),\n",
        "    'AdaBoost Regressor (Tuned)': (AdaBoostRegressor(estimator=DecisionTreeRegressor()), {'regressor__n_estimators': [100], 'regressor__learning_rate': [2.0], 'regressor__loss': ['exponential']}),\n",
        "}\n",
        "\n",
        "for regressor_name, (regressor, param_grid) in regressors.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly', PolynomialFeatures(degree=2)),\n",
        "        ('regressor', regressor)\n",
        "    ])\n",
        "    \n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "    grid_search.fit(XTrain, YTrain)\n",
        "    \n",
        "    best_models_tuned[regressor_name] = {\n",
        "        'model': grid_search.best_estimator_,\n",
        "        'params': grid_search.best_params_\n",
        "    }\n",
        "    \n",
        "\n",
        "r2_table_tuned, i = pd.DataFrame(columns=['Model Name', 'R2 Score on Training','R2 Score on Validation', '% Difference in R2']), 2\n",
        "for regressor_name, results in best_models_tuned.items():\n",
        "    YPredVal = results[\"model\"].predict(XVal)\n",
        "    YPredTrn = results[\"model\"].predict(XTrain)\n",
        "    r2_val = r2_score(YPredVal, YVal)\n",
        "    r2_trn = r2_score(YPredTrn, YTrain)\n",
        "    r2_table_tuned.loc[i] = [regressor_name, r2_trn, r2_val, abs(r2_trn-r2_val)*100/abs(r2_trn)]\n",
        "    i=i+1\n",
        "    \n",
        "r2_table_tuned"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regressors = {\n",
        "    'Decision Tree Regressor (Tuned)': (DecisionTreeRegressor(), {'regressor__max_depth': [None, 5, 10, 20], 'regressor__ccp_alpha': [0.01, 0.1, 1]}),\n",
        "    'Bagging Regressor (Tuned)': (BaggingRegressor(estimator=DecisionTreeRegressor()), {'regressor__n_estimators': [10, 50, 100]}),\n",
        "    'AdaBoost Regressor (Tuned)': (AdaBoostRegressor(estimator=DecisionTreeRegressor()), {'regressor__n_estimators': [10, 50, 100], 'regressor__learning_rate': [0.1, 1.0, 2.0], 'regressor__loss': ['linear', 'square', 'exponential']}),\n",
        "}\n",
        "\n",
        "for regressor_name, (regressor, param_grid) in regressors.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly', PolynomialFeatures(degree=2)),\n",
        "        ('regressor', regressor)\n",
        "    ])\n",
        "    \n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "    grid_search.fit(XTrain, YTrain)\n",
        "    \n",
        "    best_models_raw[regressor_name] = {\n",
        "        'model': grid_search.best_estimator_,\n",
        "        'params': grid_search.best_params_\n",
        "    }\n",
        "    \n",
        "\n",
        "r2_table_tuned, i = pd.DataFrame(columns=['Model Name', 'R2 Score on Training','R2 Score on Validation', '% Difference in R2']), 0\n",
        "for regressor_name, results in best_models_raw.items():\n",
        "    YPredVal = results[\"model\"].predict(XVal)\n",
        "    YPredTrn = results[\"model\"].predict(XTrain)\n",
        "    r2_val = r2_score(YPredVal, YVal)\n",
        "    r2_trn = r2_score(YPredTrn, YTrain)\n",
        "    r2_table_tuned.loc[i] = [regressor_name, r2_trn, r2_val, abs(r2_trn-r2_val)*100/abs(r2_trn)]\n",
        "    i=i+1\n",
        "    \n",
        "r2_table_tuned"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}